{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6aa4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import email\n",
    "import re\n",
    "from email.parser import BytesParser, Parser\n",
    "from email.policy import default\n",
    "count = 0\n",
    "# extracting months from date of email using parser object\n",
    "subjects = []\n",
    "for folder in glob.glob(\"C:\\Users\\akatn\\Desktop\\5810 PROJECT\\maildir\"):\n",
    "    #print(folder)\n",
    "    count += 1\n",
    "    if os.path.exists(os.path.join(\"C:\\Users\\akatn\\Desktop\\5810 PROJECT\\maildir\" , folder.split(\"\\\\\")[-1], \"inbox\")):\n",
    "        for textfile in glob.glob(os.path.join(\"C:\\Users\\akatn\\Desktop\\5810 PROJECT\\maildir\" , folder.split(\"\\\\\")[-1], \"inbox\", \"*\")):\n",
    "            if not os.path.isdir(textfile):\n",
    "                try:\n",
    "                    with open(textfile, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "                        data = f.read()\n",
    "                    subject = Parser(policy=default).parsestr(data.split(\"-----Original Message-----\")[0])\n",
    "                    #months.append(subject['Subject'].split(\" \")[2])   \n",
    "                    #print(subject)\n",
    "                    #if subject['Subject'] != \"\":\n",
    "                    subjects.append(subject[\"subject\"])\n",
    "                    #print(subject['subject'])\n",
    "                    #print(\"-------\")\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a1049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41509\n"
     ]
    }
   ],
   "source": [
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e90c1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \" \".join(subject for subject in subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f16a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()\n",
    "import re\n",
    "doc = re.sub('[^a-z ]', '', doc)\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stop_words = list(get_stop_words('en'))\n",
    "\n",
    "stopwords = list(set(list(STOPWORDS)) | set(list(stop_words)))\n",
    "stopwords = [re.sub('[^a-zA-Z]+', '', word) for word in stopwords]\n",
    "\n",
    "doc = \" \".join(word for word in doc.split() if word not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "764988e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in doc.split())\n",
    "\n",
    "noun_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in lemmatized_doc.split())\n",
    "adj_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"a\") for word in noun_lemmatized_doc.split())\n",
    "adv_lemmatized_doc = \" \".join(wordnet_lemmatizer.lemmatize(word, pos=\"r\") for word in adj_lemmatized_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08824bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "allWords = nltk.tokenize.word_tokenize(adv_lemmatized_doc)\n",
    "allWordDist = nltk.FreqDist(w.lower() for w in allWords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d06e327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_from_subject = {}\n",
    "for common_word in list(allWordDist):\n",
    "    common_words_from_subject[common_word] = allWordDist[common_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e401b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = []\n",
    "for word in common_words_from_subject.keys():\n",
    "    if common_words_from_subject[word] > 1000:\n",
    "        freq_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d50e6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = freq_words + [\"houston\", \"enrononline\", \"risknews\", \"announcement\", \"hotlistxls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7ba6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_doc = \" \".join(word for word in adv_lemmatized_doc.split() if word not in freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51ecbb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram - 2\n",
      "[('position westwing', 0.3719), ('risktrac whalley', 0.3717), ('expect tx', 0.3709), ('westdaypluscalxls', 0.3624), ('transco disaster', 0.359), ('line enrontradersnews', 0.3566), ('transco cornerstone', 0.3558), ('west trader', 0.355), ('ny list', 0.3538), ('friday enrontxu', 0.3526), ('west ops', 0.3472), ('list corp', 0.3468), ('portfolio west', 0.3464), ('lp emergency', 0.3463), ('conference westwide', 0.3459), ('shift western', 0.3458), ('trader west', 0.3442), ('west powergas', 0.3441), ('news csc', 0.3437), ('line corp', 0.3433), ('listweek mariner', 0.3432), ('st news', 0.3415), ('panel midwest', 0.3407), ('cgas weekend', 0.3406), ('corp daily', 0.3396), ('list trader', 0.3394), ('position friday', 0.339), ('west cso', 0.3385), ('risk mgmt', 0.3385), ('heartland steelcsn', 0.3379), ('westconnect', 0.3373), ('list west', 0.3363), ('cst west', 0.3359), ('market txu', 0.3358), ('tomorrow riskmantra', 0.3358), ('ny tccs', 0.3353), ('hetco tfs', 0.3353), ('transco ny', 0.3352), ('york mercantile', 0.3351), ('index westdaypluscalxls', 0.3349)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "for i in range(2, 3):\n",
    "    print(\"Ngram - {}\".format(i))\n",
    "    keywords = kw_model.extract_keywords(cleaned_doc, keyphrase_ngram_range=(1, i), top_n=40, stop_words=None)\n",
    "    print(keywords)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
